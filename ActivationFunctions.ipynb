{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ActivationFunctions.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabeMaldonado/-GabeMaldonado.github.io/blob/master/ActivationFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMQKN3_7R4ZP",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions\n",
        "\n",
        "## The Sigmoid Function\n",
        "Perceptrons tend to behave erraticaly. If the value of **z** is less or equal than 0 then the output value would be 0 and if the value of **z** is more than 0 -- even slightly-- its value would be one. Intuitively, we can see that this erratic behavior can affect learning. \n",
        "The **sigmoid function** provides a gentle curve from 0 to 1. This activation function moves gradually with the inputs. Small, gradual changes to **w** or **b** cause small gradual changes in **z** therefore producing small, gradual changes in the neuron's activation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c4Eo-PtGumX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V-fbKLcR2Y3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl5UMGrTG4rN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  sig = 1 / (1 + e**-z)\n",
        "  return sig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-u5hgnDHGre",
        "colab_type": "code",
        "outputId": "e08ff35a-c917-4b81-a7d8-ec7e37ce05f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sigmoid(100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjN9ZUsnHI0O",
        "colab_type": "code",
        "outputId": "e8bf2d0b-22a0-49cd-d392-ce7c55886182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sigmoid(0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6224593312018546"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ip8dVSeHLxi",
        "colab_type": "code",
        "outputId": "181adaef-a094-42a2-f7a8-eb4d8adeff08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sigmoid(0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5024999791668749"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBTN2dIEHPI2",
        "colab_type": "code",
        "outputId": "7cd01be2-c671-4093-d585-b99f4f345faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sigmoid(-0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3775406687981454"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W63vaAX2HSle",
        "colab_type": "code",
        "outputId": "84693fbb-43a5-41b6-b156-24eed0d10d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sigmoid(-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2689414213699951"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrSczQbOR1Op",
        "colab_type": "text"
      },
      "source": [
        "## The Tanh Function\n",
        "\n",
        "The Tanh function has a similar to the Sigmoid Function with the main distinction that the Sigmoid Function has an output range from 0 to 1 whereas the Tanh fucntion has an output range from -1 to 1. With negative **z** inputs the activations are negative, when **z** = 0 then the activations are 0 and when **z** is positive then the activations are also positive. This function makes its activations to be centered near 0 so neuron saturation is less likely therefore it enables the entire network to learn more efficiently. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biQ0C0GXaQNQ",
        "colab_type": "text"
      },
      "source": [
        "## The ReLU -- Rectified Linear Unit Function\n",
        "\n",
        "ReLU are the most widely used activation functions in Deep Learning. \n",
        "The shape of the ReLU functions diverges from those of the Sigmoid and Tanh function and it is defined by ```a = max(0,z)```. Basically:\n",
        "\n",
        "\n",
        "*   If **z** is a postive value, the ReLUactivation function returns **z** unadulterated as ``` a = z```\n",
        "*   If **z** = 0 or **z** is negative, the function returns its floor value of 0 that is ``` a = 0```\n",
        "\n",
        "The ReLU is basically two distinct linear functions combined, one at negative **z** values returning 0 and the other at positive **z** values returning **z**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD9iIVKZaNu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}